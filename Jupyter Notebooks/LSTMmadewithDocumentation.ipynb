{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "class Config:\n",
    "    # Date ranges\n",
    "    TRAIN_START_DATE = \"2018-05-02T08:44:39.292059872Z\"\n",
    "    TRAIN_END_DATE = \"2018-05-02T23:59:55.940964414Z\"\n",
    "    TEST_START_DATE = \"2018-05-02T23:59:55.940964414Z\"\n",
    "    TEST_END_DATE = \"2018-05-03T23:59:58.000000Z\"\n",
    "\n",
    "    # Data directory\n",
    "    DATA_DIR = r\"C:\\Users\\cinco\\Desktop\\DATA FOR SCRIPTS\\data bento data\\test\"\n",
    "\n",
    "    # Model parameters\n",
    "    BATCH_SIZE = 32\n",
    "    HIDDEN_SIZE = 64\n",
    "    NUM_LAYERS = 2\n",
    "    LEARNING_RATE = 0.001\n",
    "    EPOCHS = 2\n",
    "    PATIENCE = 10\n",
    "\n",
    "    # Training parameters\n",
    "    NUM_WORKERS = 0\n",
    "    RANDOM_SEED = 42\n",
    "\n",
    "    # Dynamic parameters\n",
    "    sequence_length = None\n",
    "    prediction_length = None\n",
    "\n",
    "    @classmethod\n",
    "    def validate_dates(cls):\n",
    "        try:\n",
    "            train_start = pd.to_datetime(cls.TRAIN_START_DATE, utc=True)\n",
    "            train_end = pd.to_datetime(cls.TRAIN_END_DATE, utc=True) + pd.Timedelta(microseconds=1)\n",
    "            test_start = pd.to_datetime(cls.TEST_START_DATE, utc=True)\n",
    "            test_end = pd.to_datetime(cls.TEST_END_DATE, utc=True) + pd.Timedelta(microseconds=1)\n",
    "            \n",
    "            print(f\"\\nValidating date ranges:\")\n",
    "            print(f\"Train period: {train_start} to {train_end}\")\n",
    "            print(f\"Test period: {test_start} to {test_end}\")\n",
    "            \n",
    "            assert train_start < train_end, \"Training start date must be before training end date\"\n",
    "            assert test_start <= test_end, \"Test start date must be before or equal to test end date\"\n",
    "            assert train_end >= test_start, \"Training end date should be at or after test start date\"\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Date validation error: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    @classmethod\n",
    "    def analyze_time_series(cls):\n",
    "        try:\n",
    "            all_diffs = []\n",
    "            csv_files = glob.glob(str(Path(cls.DATA_DIR) / \"*.csv\"))\n",
    "            \n",
    "            for file in csv_files:\n",
    "                df = pd.read_csv(file)\n",
    "                df['ts_event'] = pd.to_datetime(df['ts_event'])\n",
    "                df = df.sort_values('ts_event')\n",
    "                time_diffs = df['ts_event'].diff().dt.total_seconds()\n",
    "                all_diffs.extend(time_diffs.dropna().tolist())\n",
    "            \n",
    "            if not all_diffs:\n",
    "                raise ValueError(\"No valid time differences found in the data\")\n",
    "            \n",
    "            median_diff = np.median(all_diffs)\n",
    "            mean_diff = np.mean(all_diffs)\n",
    "            std_diff = np.std(all_diffs)\n",
    "            \n",
    "            typical_observations_per_30min = int((30 * 60) / median_diff)\n",
    "            cls.sequence_length = min(max(typical_observations_per_30min, 10), 100)\n",
    "            \n",
    "            typical_observations_per_5min = int((5 * 60) / median_diff)\n",
    "            cls.prediction_length = min(max(typical_observations_per_5min, 5), 30)\n",
    "            \n",
    "            print(f\"\\nTime Series Analysis Results:\")\n",
    "            print(f\"Median time between observations: {median_diff:.2f} seconds\")\n",
    "            print(f\"Mean time between observations: {mean_diff:.2f} seconds\")\n",
    "            print(f\"Standard deviation: {std_diff:.2f} seconds\")\n",
    "            print(f\"Selected sequence length: {cls.sequence_length} observations\")\n",
    "            print(f\"Selected prediction length: {cls.prediction_length} observations\")\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing time series: {str(e)}\")\n",
    "            cls.sequence_length = 100\n",
    "            cls.prediction_length = 30\n",
    "            return False\n",
    "\n",
    "    @classmethod\n",
    "    def initialize(cls):\n",
    "        if not cls.validate_dates():\n",
    "            raise ValueError(\"Date validation failed\")\n",
    "        if not cls.analyze_time_series():\n",
    "            print(\"Warning: Using default sequence and prediction lengths\")\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_data(directory, columns=['ts_event', 'price']):\n",
    "    data = []\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('.csv'):\n",
    "            df = pd.read_csv(os.path.join(directory, file))\n",
    "            data.append(df[columns])\n",
    "    return pd.concat(data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_df, test_df, sequence_length=60):\n",
    "    scaler = MinMaxScaler()\n",
    "    train_prices = scaler.fit_transform(train_df['price'].values.reshape(-1, 1))\n",
    "    test_prices = scaler.transform(test_df['price'].values.reshape(-1, 1))\n",
    "    \n",
    "    # Create sequences for training data\n",
    "    X_train, y_train = create_sequences(train_prices, sequence_length)\n",
    "    \n",
    "    # Create sequences for test data\n",
    "    X_test, y_test = create_sequences(test_prices, sequence_length)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, scaler\n",
    "\n",
    "def create_sequences(data, sequence_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i+sequence_length])\n",
    "        y.append(data[i+sequence_length])\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=50, num_layers=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    for epoch in range(Config.EPOCHS):\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch [{epoch+1}/{Config.EPOCHS}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_data, scaler):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(len(test_data)):\n",
    "            X = torch.FloatTensor(test_data[i]).unsqueeze(0)\n",
    "            y_pred = model(X)\n",
    "            predictions.append(scaler.inverse_transform(y_pred.numpy())[0][0])\n",
    "    \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predicted_vs_actual(df):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df['ts_event'], df['actual_price'], label='Actual Price')\n",
    "    plt.plot(df['ts_event'], df['predicted_price'], label='Predicted Price')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Price')\n",
    "    plt.title('Predicted vs Actual Prices')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate and print RMSE\n",
    "    rmse = np.sqrt(((df['actual_price'] - df['predicted_price']) ** 2).mean())\n",
    "    print(f\"Root Mean Square Error: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating date ranges:\n",
      "Train period: 2018-05-02 08:44:39.292059872+00:00 to 2018-05-02 23:59:55.940965414+00:00\n",
      "Test period: 2018-05-02 23:59:55.940964414+00:00 to 2018-05-03 23:59:58.000001+00:00\n",
      "\n",
      "Time Series Analysis Results:\n",
      "Median time between observations: 0.00 seconds\n",
      "Mean time between observations: 1.69 seconds\n",
      "Standard deviation: 27.72 seconds\n",
      "Selected sequence length: 100 observations\n",
      "Selected prediction length: 30 observations\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "preprocess_data() missing 1 required positional argument: 'test_df'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load and preprocess data\u001b[39;00m\n\u001b[0;32m      6\u001b[0m df \u001b[38;5;241m=\u001b[39m load_csv_data(Config\u001b[38;5;241m.\u001b[39mDATA_DIR)\n\u001b[1;32m----> 7\u001b[0m X, y, scaler \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequence_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Create dataset and dataloader\u001b[39;00m\n\u001b[0;32m     10\u001b[0m dataset \u001b[38;5;241m=\u001b[39m PriceDataset(X, y)\n",
      "\u001b[1;31mTypeError\u001b[0m: preprocess_data() missing 1 required positional argument: 'test_df'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize configuration\n",
    "    Config.initialize()\n",
    "\n",
    "    # Load data\n",
    "    df = load_csv_data(Config.DATA_DIR)\n",
    "    df['ts_event'] = pd.to_datetime(df['ts_event'])\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    train_mask = (df['ts_event'] >= pd.to_datetime(Config.TRAIN_START_DATE, utc=True)) & \\\n",
    "                 (df['ts_event'] <= pd.to_datetime(Config.TRAIN_END_DATE, utc=True))\n",
    "    test_mask = (df['ts_event'] >= pd.to_datetime(Config.TEST_START_DATE, utc=True)) & \\\n",
    "                (df['ts_event'] <= pd.to_datetime(Config.TEST_END_DATE, utc=True))\n",
    "\n",
    "    train_df = df[train_mask]\n",
    "    test_df = df[test_mask]\n",
    "\n",
    "    # Preprocess data\n",
    "    X_train, y_train, X_test, y_test, scaler = preprocess_data(train_df, test_df, sequence_length=Config.sequence_length)\n",
    "    \n",
    "    # Create dataset and dataloader for training data\n",
    "    train_dataset = PriceDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=Config.NUM_WORKERS)\n",
    "    \n",
    "    # Initialize model, loss function, and optimizer\n",
    "    model = LSTMModel(hidden_size=Config.HIDDEN_SIZE, num_layers=Config.NUM_LAYERS)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=Config.LEARNING_RATE)\n",
    "    \n",
    "    # Train the model\n",
    "    train_model(model, train_loader, criterion, optimizer)\n",
    "    \n",
    "    # Make predictions on test data\n",
    "    test_predictions = predict(model, X_test, scaler)\n",
    "    \n",
    "    # Create a DataFrame with test predictions\n",
    "    test_dates = test_df['ts_event'][Config.sequence_length:].reset_index(drop=True)\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'ts_event': test_dates,\n",
    "        'predicted_price': test_predictions,\n",
    "        'actual_price': scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "    })\n",
    "    \n",
    "    # Plot predicted vs actual prices\n",
    "    plot_predicted_vs_actual(predictions_df)\n",
    "    \n",
    "    print(predictions_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
