{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Date ranges\n",
    "    TRAIN_START_DATE = \"2018-05-02T08:44:39.292059872Z\"\n",
    "    TRAIN_END_DATE = \"2018-06-25T08:03:24.466039977Z\"\n",
    "    TEST_START_DATE = \"2018-06-25T08:03:24.466039977Z\"\n",
    "    TEST_END_DATE = \"2018-06-28T23:56:46.421875446Z\"\n",
    "\n",
    "    # Data files\n",
    "    DATA_DIR = r\"C:\\Users\\cinco\\Desktop\\DATA FOR SCRIPTS\\data bento data\\test2\"\n",
    "    FILE_PREFIX = \"xnas.itch_NVDA_\"\n",
    "    FILE_SUFFIX = \".csv\"\n",
    "\n",
    "    # Model parameters\n",
    "    BATCH_SIZE = 32\n",
    "    HIDDEN_SIZE = 64\n",
    "    NUM_LAYERS = 2\n",
    "    LEARNING_RATE = 0.001\n",
    "    EPOCHS = 2\n",
    "    SEQUENCE_LENGTH = 100\n",
    "    PREDICTION_LENGTH = 30\n",
    "\n",
    "    # Training parameters\n",
    "    NUM_WORKERS = 0\n",
    "    RANDOM_SEED = 42\n",
    "\n",
    "    @classmethod\n",
    "    def validate_dates(cls):\n",
    "        try:\n",
    "            train_start = pd.to_datetime(cls.TRAIN_START_DATE, utc=True)\n",
    "            train_end = pd.to_datetime(cls.TRAIN_END_DATE, utc=True)\n",
    "            test_start = pd.to_datetime(cls.TEST_START_DATE, utc=True)\n",
    "            test_end = pd.to_datetime(cls.TEST_END_DATE, utc=True)\n",
    "            \n",
    "            print(f\"\\nValidating date ranges:\")\n",
    "            print(f\"Train period: {train_start} to {train_end}\")\n",
    "            print(f\"Test period: {test_start} to {test_end}\")\n",
    "            \n",
    "            assert train_start < train_end, \"Training start date must be before training end date\"\n",
    "            assert test_start < test_end, \"Test start date must be before test end date\"\n",
    "            assert train_end <= test_start, \"Training end date should be before or equal to test start date\"\n",
    "            \n",
    "            # Adjust test_start if it's equal to train_end\n",
    "            if train_end == test_start:\n",
    "                cls.TEST_START_DATE = (test_start + pd.Timedelta(microseconds=1)).isoformat()\n",
    "                print(f\"Adjusted test start date to: {cls.TEST_START_DATE}\")\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Date validation error: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    @classmethod\n",
    "    def analyze_time_series(cls):\n",
    "        try:\n",
    "            all_diffs = []\n",
    "            csv_files = glob.glob(str(Path(cls.DATA_DIR) / \"*.csv\"))\n",
    "            \n",
    "            for file in csv_files:\n",
    "                df = pd.read_csv(file)\n",
    "                df['ts_event'] = pd.to_datetime(df['ts_event'])\n",
    "                df = df.sort_values('ts_event')\n",
    "                time_diffs = df['ts_event'].diff().dt.total_seconds()\n",
    "                all_diffs.extend(time_diffs.dropna().tolist())\n",
    "            \n",
    "            if not all_diffs:\n",
    "                raise ValueError(\"No valid time differences found in the data\")\n",
    "            \n",
    "            median_diff = np.median(all_diffs)\n",
    "            mean_diff = np.mean(all_diffs)\n",
    "            std_diff = np.std(all_diffs)\n",
    "            \n",
    "            typical_observations_per_30min = int((30 * 60) / median_diff)\n",
    "            cls.SEQUENCE_LENGTH = min(max(typical_observations_per_30min, 10), 100)\n",
    "            \n",
    "            typical_observations_per_5min = int((5 * 60) / median_diff)\n",
    "            cls.PREDICTION_LENGTH = min(max(typical_observations_per_5min, 5), 30)\n",
    "            \n",
    "            print(f\"\\nTime Series Analysis Results:\")\n",
    "            print(f\"Median time between observations: {median_diff:.2f} seconds\")\n",
    "            print(f\"Mean time between observations: {mean_diff:.2f} seconds\")\n",
    "            print(f\"Standard deviation: {std_diff:.2f} seconds\")\n",
    "            print(f\"Selected sequence length: {cls.SEQUENCE_LENGTH} observations\")\n",
    "            print(f\"Selected prediction length: {cls.PREDICTION_LENGTH} observations\")\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing time series: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    @classmethod\n",
    "    def get_file_list(cls):\n",
    "        start_date = pd.to_datetime(cls.TRAIN_START_DATE).date()\n",
    "        end_date = pd.to_datetime(cls.TEST_END_DATE).date()\n",
    "        date_range = pd.date_range(start_date, end_date)\n",
    "        \n",
    "        file_list = []\n",
    "        for date in date_range:\n",
    "            file_name = f\"{cls.FILE_PREFIX}{date.strftime('%Y%m%d')}_to_{(date + timedelta(days=1)).strftime('%Y%m%d')}{cls.FILE_SUFFIX}\"\n",
    "            file_path = os.path.join(cls.DATA_DIR, file_name)\n",
    "            if os.path.exists(file_path):\n",
    "                file_list.append(file_path)\n",
    "        \n",
    "        return file_list\n",
    "\n",
    "    @classmethod\n",
    "    def initialize(cls):\n",
    "        if not cls.validate_dates():\n",
    "            raise ValueError(\"Date validation failed\")\n",
    "        if not cls.analyze_time_series():\n",
    "            print(\"Warning: Using default sequence and prediction lengths\")\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_data(file_list, columns=['ts_event', 'price']):\n",
    "    dfs = []\n",
    "    for file in file_list:\n",
    "        df = pd.read_csv(file, usecols=columns)\n",
    "        dfs.append(df)\n",
    "    \n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    df['ts_event'] = pd.to_datetime(df['ts_event'], utc=True)\n",
    "    return df.sort_values('ts_event')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_progress():\n",
    "    # Initialize config\n",
    "    Config.initialize()\n",
    "\n",
    "    # Get file list\n",
    "    file_list = Config.get_file_list()\n",
    "\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    df = load_csv_data(file_list)\n",
    "\n",
    "    # Preprocess data\n",
    "    print(\"Preprocessing data...\")\n",
    "    train_start = pd.to_datetime(Config.TRAIN_START_DATE, utc=True)\n",
    "    train_end = pd.to_datetime(Config.TRAIN_END_DATE, utc=True)\n",
    "    test_start = pd.to_datetime(Config.TEST_START_DATE, utc=True)\n",
    "    test_end = pd.to_datetime(Config.TEST_END_DATE, utc=True)\n",
    "\n",
    "    X_train, y_train, X_test, y_test, scaler, test_df = preprocess_data(\n",
    "        df, train_start, train_end, test_start, test_end, Config.SEQUENCE_LENGTH\n",
    "    )\n",
    "\n",
    "    # Create dataset and dataloader for training data\n",
    "    train_dataset = PriceDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=Config.NUM_WORKERS)\n",
    "\n",
    "    # Initialize model, loss function, and optimizer\n",
    "    model = LSTMModel(hidden_size=Config.HIDDEN_SIZE, num_layers=Config.NUM_LAYERS)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=Config.LEARNING_RATE)\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training model...\")\n",
    "    train_model_with_progress(model, train_loader, criterion, optimizer)\n",
    "\n",
    "    # Make predictions on test data\n",
    "    print(\"Making predictions...\")\n",
    "    test_predictions = predict_with_progress(model, X_test, scaler)\n",
    "\n",
    "    # Create a DataFrame with test predictions\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'ts_event': test_df['ts_event'][Config.SEQUENCE_LENGTH:].reset_index(drop=True),\n",
    "        'predicted_price': test_predictions,\n",
    "        'actual_price': test_df['price'][Config.SEQUENCE_LENGTH:].reset_index(drop=True)\n",
    "    })\n",
    "\n",
    "    # Plot predicted vs actual prices\n",
    "    plot_predicted_vs_actual(predictions_df)\n",
    "\n",
    "    print(predictions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, train_start, train_end, test_start, test_end, sequence_length):\n",
    "    train_mask = (df['ts_event'] >= train_start) & (df['ts_event'] < train_end)\n",
    "    test_mask = (df['ts_event'] >= test_start) & (df['ts_event'] <= test_end)\n",
    "\n",
    "    train_df = df[train_mask]\n",
    "    test_df = df[test_mask]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    train_prices = scaler.fit_transform(train_df['price'].values.reshape(-1, 1))\n",
    "    test_prices = scaler.transform(test_df['price'].values.reshape(-1, 1))\n",
    "\n",
    "    X_train, y_train = create_sequences(train_prices, sequence_length)\n",
    "    X_test, y_test = create_sequences(test_prices, sequence_length)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, scaler, test_df\n",
    "\n",
    "def create_sequences(data, sequence_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i+sequence_length])\n",
    "        y.append(data[i+sequence_length])\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=50, num_layers=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_progress(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    for epoch in range(Config.EPOCHS):\n",
    "        print(f\"Epoch [{epoch+1}/{Config.EPOCHS}]\")\n",
    "        pbar = tqdm(total=len(train_loader), unit=\"batch\")\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "        pbar.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_progress(model, test_data, scaler):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(total=len(test_data), unit=\"prediction\")\n",
    "        for i in range(len(test_data)):\n",
    "            X = torch.FloatTensor(test_data[i]).unsqueeze(0)\n",
    "            y_pred = model(X)\n",
    "            predictions.append(scaler.inverse_transform(y_pred.numpy())[0][0])\n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "    \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_data(df, train_start, train_end):\n",
    "    train_df = df[(df['ts_event'] >= train_start) & (df['ts_event'] < train_end)]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(train_df['ts_event'], train_df['price'], label='Actual Price (Train)')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Price')\n",
    "    plt.title('Training Data: Actual Prices')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4012206134.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[11], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    plt.plot(df['ts_event'],def plot_predicted_vs_actual(df, test_start, test_end):\u001b[0m\n\u001b[1;37m                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def plot_predicted_vs_actual(df, test_start, test_end):\n",
    "    test_df = df[(df['ts_event'] >= test_start) & (df['ts_event'] <= test_end)]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(test_df['ts_event'], test_df['actual_price'], label='Actual Price (Test)')\n",
    "    plt.plot(test_df['ts_event'], test_df['predicted_price'], label='Predicted Price')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Price')\n",
    "    plt.title('Test Data: Predicted vs Actual Prices')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    rmse = np.sqrt(((test_df['actual_price'] - test_df['predicted_price']) ** 2).mean())\n",
    "    print(f\"Root Mean Square Error: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize config\n",
    "    Config.initialize()\n",
    "\n",
    "    # Get file list\n",
    "    file_list = Config.get_file_list()\n",
    "\n",
    "    # Load data\n",
    "    df = load_csv_data(file_list)\n",
    "\n",
    "    # Preprocess data\n",
    "    train_start = pd.to_datetime(Config.TRAIN_START_DATE, utc=True)\n",
    "    train_end = pd.to_datetime(Config.TRAIN_END_DATE, utc=True)\n",
    "    test_start = pd.to_datetime(Config.TEST_START_DATE, utc=True)\n",
    "    test_end = pd.to_datetime(Config.TEST_END_DATE, utc=True)\n",
    "\n",
    "    # Plot training data\n",
    "    plot_train_data(df, train_start, train_end)\n",
    "\n",
    "    X_train, y_train, X_test, y_test, scaler, test_df = preprocess_data(\n",
    "        df, train_start, train_end, test_start, test_end, Config.SEQUENCE_LENGTH\n",
    "    )\n",
    "\n",
    "    # Create dataset and dataloader for training data\n",
    "    train_dataset = PriceDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=Config.NUM_WORKERS)\n",
    "    \n",
    "    # Initialize model, loss function, and optimizer\n",
    "    model = LSTMModel(hidden_size=Config.HIDDEN_SIZE, num_layers=Config.NUM_LAYERS)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=Config.LEARNING_RATE)\n",
    "    \n",
    "    # Train the model\n",
    "    train_model_with_progress(model, train_loader, criterion, optimizer)\n",
    "    \n",
    "    # Make predictions on test data\n",
    "    test_predictions = predict_with_progress(model, X_test, scaler)\n",
    "    \n",
    "    # Create a DataFrame with test predictions\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'ts_event': test_df['ts_event'][Config.SEQUENCE_LENGTH:].reset_index(drop=True),\n",
    "        'predicted_price': test_predictions,\n",
    "        'actual_price': test_df['price'][Config.SEQUENCE_LENGTH:].reset_index(drop=True)\n",
    "    })\n",
    "    \n",
    "    # Plot predicted vs actual prices for test data\n",
    "    plot_predicted_vs_actual(predictions_df, test_start, test_end)\n",
    "    \n",
    "    print(predictions_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
