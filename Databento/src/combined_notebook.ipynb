{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "676e3023",
   "metadata": {},
   "source": [
    "# File: config\\config.py\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34e8da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    TRAIN_START_DATE = \"2018-05-02T08:44:39.292071841Z\"\n",
    "    TRAIN_END_DATE = \"2024-10-21T08:00:00.143539165Z\"\n",
    "    TEST_START_DATE = \"2024-10-21T08:00:00.143539165Z\"\n",
    "    TEST_END_DATE = \"2024-10-21T23:59:51.581344604Z\"\n",
    "    DATA_DIR = r\"C:\\Users\\cinco\\Desktop\\DATA FOR SCRIPTS\\data bento data\\NVDA\"  # Change to your production data directory\n",
    "    \n",
    "    # Model parameters\n",
    "    SEQUENCE_LENGTH = 1440\n",
    "    PREDICTION_LENGTH = 60\n",
    "    BATCH_SIZE = 128\n",
    "    HIDDEN_SIZE = 256\n",
    "    NUM_LAYERS = 4\n",
    "    LEARNING_RATE = 0.001\n",
    "    EPOCHS = 50 # Increased from 30 to 100\n",
    "    PATIENCE = 50  # Early stopping patience\n",
    "    \n",
    "    # Training parameters\n",
    "    TRAIN_VAL_SPLIT = 0.8\n",
    "    NUM_WORKERS = 4\n",
    "    \n",
    "    # Other parameters\n",
    "    RANDOM_SEED = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af0412e",
   "metadata": {},
   "source": [
    "# File: data\\__init__.py\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f8c276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e3c8d29",
   "metadata": {},
   "source": [
    "# File: data\\data_loader.py\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa44c07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "def process_csv_file(file_path, start_date, end_date):\n",
    "    \"\"\"Process a single CSV file and return cleaned DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the CSV file\n",
    "        start_date: Start date for filtering data\n",
    "        end_date: End date for filtering data\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with ts_event and price columns\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, parse_dates=['ts_event'], dtype={'price': 'float32'})\n",
    "        # Filter data between start and end dates\n",
    "        mask = (df['ts_event'] >= start_date) & (df['ts_event'] <= end_date)\n",
    "        df = df[mask]\n",
    "        if not df.empty:\n",
    "            df = df[['ts_event', 'price']].set_index('ts_event')\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame on error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a714d98d",
   "metadata": {},
   "source": [
    "# File: data\\dataset.py\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c051e603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class TimeSeriesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, sequence_length, prediction_length, scaler):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_length = prediction_length\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.sequence_length - self.prediction_length + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get sequence\n",
    "        X = self.data[idx:idx + self.sequence_length]\n",
    "        # Get target (next prediction_length values)\n",
    "        y = self.data[idx + self.sequence_length:idx + self.sequence_length + self.prediction_length, 0]\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81627de",
   "metadata": {},
   "source": [
    "# File: main.py\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4c1e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from config.config import Config\n",
    "from data.data_loader import process_csv_file\n",
    "from data.dataset import TimeSeriesDataset\n",
    "from models.lstm import PricePredictionLSTM\n",
    "from utils.preprocessing import add_technical_features\n",
    "from utils.visualization import create_price_plot\n",
    "from training.trainer import train_model\n",
    "from sklearn.preprocessing import MinMaxScaler  # For data_scaler\n",
    "from torch.utils.data import DataLoader        # For DataLoader\n",
    "import torch.nn as nn                          # For nn.MSELoss()\n",
    "import torch.optim as optim                    # For optim.Adam()\n",
    "def main():\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(Config.RANDOM_SEED)\n",
    "    np.random.seed(Config.RANDOM_SEED)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load and process training data\n",
    "    train_data = []\n",
    "    csv_files = glob.glob(str(Path(Config.DATA_DIR) / \"*.csv\"))\n",
    "    \n",
    "    if not csv_files:\n",
    "        raise ValueError(f\"No CSV files found in directory: {Config.DATA_DIR}\")\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files\")\n",
    "    print(\"Processing training data...\")\n",
    "    \n",
    "    for file in tqdm(csv_files):\n",
    "        df = process_csv_file(file, Config.TRAIN_START_DATE, Config.TRAIN_END_DATE)\n",
    "        if not df.empty:\n",
    "            train_data.append(df)\n",
    "    \n",
    "    if not train_data:\n",
    "        raise ValueError(\n",
    "            f\"No training data found between {Config.TRAIN_START_DATE} and {Config.TRAIN_END_DATE}\\n\"\n",
    "            f\"Please check your date ranges and data directory: {Config.DATA_DIR}\"\n",
    "        )\n",
    "    \n",
    "    train_df = pd.concat(train_data)\n",
    "    train_df = train_df.sort_index()\n",
    "    \n",
    "    # Load and process test data\n",
    "    test_data = []\n",
    "    print(\"Processing test data...\")\n",
    "    \n",
    "    for file in tqdm(csv_files):\n",
    "        df = process_csv_file(file, Config.TEST_START_DATE, Config.TEST_END_DATE)\n",
    "        if not df.empty:\n",
    "            test_data.append(df)\n",
    "    \n",
    "    if not test_data:\n",
    "        raise ValueError(\n",
    "            f\"No test data found between {Config.TEST_START_DATE} and {Config.TEST_END_DATE}\\n\"\n",
    "            f\"Please check your date ranges and data directory: {Config.DATA_DIR}\"\n",
    "        )\n",
    "    \n",
    "    test_df = pd.concat(test_data)\n",
    "    test_df = test_df.sort_index()\n",
    "    \n",
    "    # Print data information\n",
    "    print(\"\\nData Summary:\")\n",
    "    print(f\"Training data shape: {train_df.shape}\")\n",
    "    print(f\"Training date range: {train_df.index.min()} to {train_df.index.max()}\")\n",
    "    print(f\"Test data shape: {test_df.shape}\")\n",
    "    print(f\"Test date range: {test_df.index.min()} to {test_df.index.max()}\")\n",
    "\n",
    "    # Combine data for feature engineering\n",
    "    full_df = pd.concat([train_df, test_df])\n",
    "    full_df = full_df.resample('1min').last().ffill()    \n",
    "    full_df = full_df.sort_index()\n",
    "\n",
    "    print(\"\\nFull dataset shape:\", full_df.shape)\n",
    "    print(f\"Full date range: {full_df.index.min()} to {full_df.index.max()}\")\n",
    "    \n",
    "    print(\"Resampling to minute intervals...\")\n",
    "    actual_data = pd.DataFrame({\n",
    "        'timestamp': full_df.index,\n",
    "        'actual_price': full_df['price']\n",
    "    })\n",
    "    \n",
    "    print(\"Adding technical features...\")\n",
    "    full_df = add_technical_features(full_df)\n",
    "    \n",
    "    print(\"Scaling data...\")\n",
    "    data_scaler = MinMaxScaler()\n",
    "    full_df = full_df.astype('float32')\n",
    "    scaled_data = data_scaler.fit_transform(full_df.values)\n",
    "    \n",
    "    # Split data based on dates\n",
    "    train_mask = (full_df.index >= Config.TRAIN_START_DATE) & (full_df.index <= Config.TRAIN_END_DATE)\n",
    "    train_data = scaled_data[train_mask]\n",
    "    test_data = scaled_data[~train_mask]\n",
    "    \n",
    "    # Create datasets\n",
    "    train_size = int(Config.TRAIN_VAL_SPLIT * len(train_data))\n",
    "    train_dataset = TimeSeriesDataset(train_data[:train_size], \n",
    "                                    Config.SEQUENCE_LENGTH, \n",
    "                                    Config.PREDICTION_LENGTH, \n",
    "                                    data_scaler)\n",
    "    val_dataset = TimeSeriesDataset(train_data[train_size:], \n",
    "                                  Config.SEQUENCE_LENGTH, \n",
    "                                  Config.PREDICTION_LENGTH, \n",
    "                                  data_scaler)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=Config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=Config.NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=Config.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=Config.NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    input_size = scaled_data.shape[1]\n",
    "    model = PricePredictionLSTM(\n",
    "        input_size=input_size,\n",
    "        hidden_size=Config.HIDDEN_SIZE,\n",
    "        num_layers=Config.NUM_LAYERS,\n",
    "        output_size=Config.PREDICTION_LENGTH\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=Config.LEARNING_RATE)\n",
    "    \n",
    "    print(f\"Starting training for {Config.EPOCHS} epochs...\")\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                Config.EPOCHS, device, Config.PATIENCE)\n",
    "    \n",
    "# [Previous code remains the same until the prediction generation part]\n",
    "\n",
    "    print(\"Generating predictions...\")\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        last_sequence = scaled_data[-Config.SEQUENCE_LENGTH:].reshape(1, Config.SEQUENCE_LENGTH, -1)\n",
    "        last_sequence = torch.FloatTensor(last_sequence).to(device)\n",
    "        predictions = model(last_sequence)\n",
    "        predictions = predictions.cpu().numpy()\n",
    "    \n",
    "    # Process predictions\n",
    "    predictions_reshaped = np.zeros((len(predictions[0]), scaled_data.shape[1]))\n",
    "    predictions_reshaped[:, 0] = predictions[0]\n",
    "    predicted_prices = data_scaler.inverse_transform(predictions_reshaped)[:, 0]\n",
    "    \n",
    "    # Create timestamps for predictions\n",
    "    last_timestamp = pd.Timestamp(Config.TEST_END_DATE)\n",
    "    pred_index = pd.date_range(\n",
    "        start=last_timestamp, \n",
    "        periods=Config.PREDICTION_LENGTH + 1, \n",
    "        freq='1T'\n",
    "    )[1:]\n",
    "    \n",
    "    # Create predictions DataFrame\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'timestamp': pred_index,\n",
    "        'price': predicted_prices\n",
    "    })\n",
    "    \n",
    "    # Get the actual prices for comparison\n",
    "    last_actual_prices = actual_data[\n",
    "        (actual_data['timestamp'] <= last_timestamp) & \n",
    "        (actual_data['timestamp'] > last_timestamp - pd.Timedelta(minutes=Config.PREDICTION_LENGTH))\n",
    "    ].copy()\n",
    "    \n",
    "    last_actual_prices = last_actual_prices.rename(columns={'actual_price': 'price'})\n",
    "    \n",
    "    # Create comparison DataFrame by concatenating along the index\n",
    "    comparison_df = pd.concat(\n",
    "        [last_actual_prices, predictions_df],\n",
    "        axis=0,\n",
    "        ignore_index=True\n",
    "    ).sort_values('timestamp').reset_index(drop=True)\n",
    "    \n",
    "    # Add a type column to distinguish between actual and predicted values\n",
    "    comparison_df['type'] = 'predicted'\n",
    "    comparison_df.loc[comparison_df['timestamp'] <= last_timestamp, 'type'] = 'actual'\n",
    "    \n",
    "    # Create separate DataFrames for saving\n",
    "    predictions_to_save = predictions_df.copy()\n",
    "    predictions_to_save = predictions_to_save.rename(columns={'price': 'predicted_price'})\n",
    "    predictions_to_save.to_csv('price_predictions.csv', index=False)\n",
    "    \n",
    "    comparison_to_save = comparison_df.pivot(\n",
    "        index='timestamp',\n",
    "        columns='type',\n",
    "        values='price'\n",
    "    ).reset_index()\n",
    "    comparison_to_save.to_csv('price_comparison.csv', index=False)\n",
    "    \n",
    "    print(\"Predictions saved to price_predictions.csv\")\n",
    "    print(\"Comparison data saved to price_comparison.csv\")\n",
    "    \n",
    "    # Create and save the plot\n",
    "    plot_path = create_price_plot(comparison_df, last_timestamp)\n",
    "    print(f\"Price prediction plot saved to {plot_path}\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    actual_mask = comparison_df['type'] == 'actual'\n",
    "    predicted_mask = comparison_df['type'] == 'predicted'\n",
    "    \n",
    "    # Find overlapping timestamps\n",
    "    overlap_timestamps = set(comparison_df[actual_mask]['timestamp']) & set(comparison_df[predicted_mask]['timestamp'])\n",
    "    \n",
    "    if overlap_timestamps:\n",
    "        overlap_df = comparison_df[comparison_df['timestamp'].isin(overlap_timestamps)].copy()\n",
    "        actual_values = overlap_df[actual_mask]['price'].values\n",
    "        predicted_values = overlap_df[predicted_mask]['price'].values\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mse = np.mean((actual_values - predicted_values) ** 2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = np.mean(np.abs(actual_values - predicted_values))\n",
    "        mape = np.mean(np.abs((actual_values - predicted_values) / actual_values)) * 100\n",
    "        \n",
    "        print(\"\\nPrediction Statistics for Overlap Period:\")\n",
    "        print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "        print(f\"Root Mean Squared Error: {rmse:.4f}\")\n",
    "        print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "        print(f\"Mean Absolute Percentage Error: {mape:.2f}%\")\n",
    "        # Optional: Check for overlap\n",
    "        if not (train_df.index.max() < test_df.index.min()):\n",
    "            warnings.warn(\"Warning: Overlap detected between training and test data!\")\n",
    "        # Additional analysis\n",
    "        print(\"\\nPrice Range Analysis:\")\n",
    "        print(f\"Actual Price Range: {np.min(actual_values):.2f} to {np.max(actual_values):.2f}\")\n",
    "        print(f\"Predicted Price Range: {np.min(predicted_values):.2f} to {np.max(predicted_values):.2f}\")\n",
    "        \n",
    "        # Calculate correlation if there are enough points\n",
    "        if len(actual_values) > 1:\n",
    "            correlation = np.corrcoef(actual_values, predicted_values)[0, 1]\n",
    "            print(f\"\\nCorrelation between actual and predicted prices: {correlation:.4f}\")\n",
    "        \n",
    "        # Save detailed statistics\n",
    "        stats_df = pd.DataFrame({\n",
    "            'timestamp': list(overlap_timestamps),\n",
    "            'actual_price': actual_values,\n",
    "            'predicted_price': predicted_values,\n",
    "            'absolute_error': np.abs(actual_values - predicted_values),\n",
    "            'percentage_error': np.abs((actual_values - predicted_values) / actual_values) * 100\n",
    "        })\n",
    "        \n",
    "        stats_df.to_csv('prediction_statistics.csv', index=False)\n",
    "        print(\"\\nDetailed statistics saved to prediction_statistics.csv\")\n",
    "    else:\n",
    "        print(\"\\nNo overlap period found between actual and predicted prices.\")\n",
    "        print(\"This might occur if the prediction period starts after all actual data points.\")\n",
    "        print(f\"Last actual timestamp: {last_actual_prices['timestamp'].max()}\")\n",
    "        print(f\"First prediction timestamp: {predictions_df['timestamp'].min()}\")\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {str(e)}\")\n",
    "        import traceback\n",
    "        print(\"\\nFull traceback:\")\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0b5779",
   "metadata": {},
   "source": [
    "# File: models\\__init__.py\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e71cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00a6ac76",
   "metadata": {},
   "source": [
    "# File: models\\lstm.py\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172a95ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PricePredictionLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(PricePredictionLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "            \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d0c11b",
   "metadata": {},
   "source": [
    "# File: training\\__init__.py\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a1c801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "863f6e35",
   "metadata": {},
   "source": [
    "# File: training\\trainer.py\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7dffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, patience=50):\n",
    "    \"\"\"Train the model with early stopping.\"\"\"\n",
    "    best_val_loss = float('inf')\n",
    "    no_improve = 0\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Training Progress\"):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for X_batch, y_batch in tqdm(train_loader, leave=False, desc=f\"Epoch {epoch}\"):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                y_pred = model(X_batch)\n",
    "                val_loss += criterion(y_pred, y_batch).item()\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bec2edc",
   "metadata": {},
   "source": [
    "# File: utils\\__init__.py\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9048ebff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09a237e4",
   "metadata": {},
   "source": [
    "# File: utils\\preprocessing.py\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d59d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "def add_technical_features(df):\n",
    "    \"\"\"Add technical indicators as features.\"\"\"\n",
    "    df = df.astype('float32')\n",
    "    \n",
    "    df['returns'] = df['price'].pct_change()\n",
    "    \n",
    "    windows = [5, 15, 30, 60]\n",
    "    for window in windows:\n",
    "        df[f'sma_{window}'] = df['price'].rolling(window=window).mean()\n",
    "        df[f'std_{window}'] = df['price'].rolling(window=window).std()\n",
    "    \n",
    "    df['hour'] = df.index.hour.astype('float32')\n",
    "    df['minute'] = df.index.minute.astype('float32')\n",
    "    df['day_of_week'] = df.index.dayofweek.astype('float32')\n",
    "    \n",
    "    df = df.ffill().bfill()    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d047006",
   "metadata": {},
   "source": [
    "# File: utils\\visualization.py\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bf01ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def format_price(x, p):\n",
    "    \"\"\"Format price values for the y-axis\"\"\"\n",
    "    return f'${x:,.2f}'\n",
    "\n",
    "\n",
    "def create_price_plot(comparison_df, last_timestamp, save_path='price_prediction_plot.png'):\n",
    "    \"\"\"Create and save a plot of actual vs predicted prices\"\"\"\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Convert timestamp to datetime if it's not already\n",
    "    comparison_df['timestamp'] = pd.to_datetime(comparison_df['timestamp'])\n",
    "    \n",
    "    # Plot actual prices\n",
    "    actual_mask = comparison_df['type'] == 'actual'\n",
    "    if any(actual_mask):\n",
    "        plt.plot(comparison_df[actual_mask]['timestamp'], \n",
    "                comparison_df[actual_mask]['price'],\n",
    "                label='Actual Price', \n",
    "                color='blue',\n",
    "                linewidth=2)\n",
    "    \n",
    "    # Plot predicted prices\n",
    "    predicted_mask = comparison_df['type'] == 'predicted'\n",
    "    if any(predicted_mask):\n",
    "        plt.plot(comparison_df[predicted_mask]['timestamp'], \n",
    "                comparison_df[predicted_mask]['price'],\n",
    "                label='Predicted Price', \n",
    "                color='red',\n",
    "                linewidth=2,\n",
    "                linestyle='--')\n",
    "    \n",
    "    # Add vertical line at cutoff point\n",
    "    plt.axvline(x=last_timestamp, color='gray', linestyle=':', label='Prediction Start')\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.title('Price Prediction Analysis', fontsize=16, pad=20)\n",
    "    plt.xlabel('Time', fontsize=12)\n",
    "    plt.ylabel('Price ($)', fontsize=12)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Format x-axis\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))\n",
    "    plt.gcf().autofmt_xdate()  # Rotate and align x-axis labels\n",
    "    \n",
    "    # Format y-axis\n",
    "    plt.gca().yaxis.set_major_formatter(FuncFormatter(format_price))\n",
    "    \n",
    "    # Add padding to the layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return save_path"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
