{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler  # For data_scaler\n",
    "from torch.utils.data import DataLoader        # For DataLoader\n",
    "import torch.nn as nn                          # For nn.MSELoss()\n",
    "import torch.optim as optim                    # For optim.Adam()\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "import traceback  # Add at the top with other imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST ON DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def analyze_data_timestamps(data_dir):\n",
    "    \"\"\"\n",
    "    Analyze timestamp ranges in all CSV files in the directory.\n",
    "    \n",
    "    Args:\n",
    "        data_dir (str): Path to directory containing CSV files\n",
    "    \"\"\"\n",
    "    try:\n",
    "        csv_files = glob.glob(str(Path(data_dir) / \"*.csv\"))\n",
    "        if not csv_files:\n",
    "            print(f\"No CSV files found in {data_dir}\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nFound {len(csv_files)} files. Analyzing timestamps...\")\n",
    "        \n",
    "        for file in csv_files:\n",
    "            try:\n",
    "                # Read file\n",
    "                df = pd.read_csv(file)\n",
    "                df['ts_event'] = pd.to_datetime(df['ts_event'], utc=True)\n",
    "                \n",
    "                # Get timestamp range\n",
    "                min_ts = df['ts_event'].min()\n",
    "                max_ts = df['ts_event'].max()\n",
    "                record_count = len(df)\n",
    "                \n",
    "                # Print results\n",
    "                print(f\"\\nFile: {Path(file).name}\")\n",
    "                print(f\"Records: {record_count:,}\")\n",
    "                print(f\"Time range: {min_ts} to {max_ts}\")\n",
    "                \n",
    "                # Print sample of data points\n",
    "                print(\"\\nSample timestamps (first 5):\")\n",
    "                for ts in df['ts_event'].head().dt.strftime('%Y-%m-%dT%H:%M:%S.%fZ'):\n",
    "                    print(ts)\n",
    "                    \n",
    "                # Optional: analyze gaps\n",
    "                time_diffs = df['ts_event'].diff()\n",
    "                max_gap = time_diffs.max()\n",
    "                if max_gap.total_seconds() > 60:  # Show gaps larger than 1 minute\n",
    "                    gap_idx = time_diffs.idxmax()\n",
    "                    gap_start = df['ts_event'].iloc[gap_idx-1]\n",
    "                    gap_end = df['ts_event'].iloc[gap_idx]\n",
    "                    print(f\"\\nLargest gap: {max_gap}\")\n",
    "                    print(f\"Gap period: {gap_start} to {gap_end}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {Path(file).name}: {str(e)}\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing directory: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Use the function like this:\n",
    "data_dir = r\"C:\\Users\\cinco\\Desktop\\DATA FOR SCRIPTS\\data bento data\\test\"\n",
    "analyze_data_timestamps(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Config:\n",
    "    # Training on May 3rd data\n",
    "    TRAIN_START_DATE = \"2018-05-03T08:00:46.000000Z\"\n",
    "    TRAIN_END_DATE = \"2018-05-03T20:00:00.000000Z\"  # Use first part for training\n",
    "    \n",
    "    # Testing on remaining May 3rd data\n",
    "    TEST_START_DATE = \"2018-05-03T20:00:00.000000Z\"\n",
    "    TEST_END_DATE = \"2018-05-03T23:59:58.000000Z\"\n",
    "    \n",
    "\n",
    "    DATA_DIR = r\"C:\\Users\\cinco\\Desktop\\DATA FOR SCRIPTS\\data bento data\\test\"\n",
    "    \n",
    "    # Model parameters\n",
    "    BATCH_SIZE = 32\n",
    "    HIDDEN_SIZE = 64\n",
    "    NUM_LAYERS = 2\n",
    "    LEARNING_RATE = 0.001\n",
    "    EPOCHS = 50\n",
    "    PATIENCE = 10\n",
    "    \n",
    "    # Training parameters\n",
    "    TRAIN_VAL_SPLIT = 0.8\n",
    "    NUM_WORKERS = 0\n",
    "    RANDOM_SEED = 42\n",
    "    \n",
    "    # Dynamic parameters (to be set during runtime)\n",
    "    sequence_length = None\n",
    "    prediction_length = None\n",
    "    \n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def validate_dates(cls):\n",
    "        \"\"\"Validate the configuration dates\"\"\"\n",
    "        try:\n",
    "            # Add small buffer to avoid exact matches\n",
    "            train_start = pd.to_datetime(cls.TRAIN_START_DATE, utc=True)\n",
    "            train_end = pd.to_datetime(cls.TRAIN_END_DATE, utc=True) + pd.Timedelta(microseconds=1)\n",
    "            test_start = pd.to_datetime(cls.TEST_START_DATE, utc=True)\n",
    "            test_end = pd.to_datetime(cls.TEST_END_DATE, utc=True) + pd.Timedelta(microseconds=1)\n",
    "            \n",
    "            print(f\"\\nValidating date ranges:\")\n",
    "            print(f\"Train period: {train_start} to {train_end}\")\n",
    "            print(f\"Test period: {test_start} to {test_end}\")\n",
    "            \n",
    "            assert train_start < train_end, \"Training start date must be before training end date\"\n",
    "            assert test_start <= test_end, \"Test start date must be before or equal to test end date\"\n",
    "            assert train_end >= test_start, \"Training end date should be at or after test start date\"\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Date validation error: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    @classmethod\n",
    "    def analyze_time_series(cls, data_dir=None):\n",
    "        \"\"\"Analyze time series data to determine appropriate sequence and prediction lengths.\"\"\"\n",
    "        try:\n",
    "            import pandas as pd\n",
    "            import glob\n",
    "            from pathlib import Path\n",
    "            import numpy as np\n",
    "            \n",
    "            # If no data_dir provided, use the class's DATA_DIR\n",
    "            if data_dir is None:\n",
    "                data_dir = cls.DATA_DIR\n",
    "                \n",
    "            all_diffs = []\n",
    "            csv_files = glob.glob(str(Path(data_dir) / \"*.csv\"))\n",
    "            \n",
    "            # Process all CSV files to analyze time differences\n",
    "            for file in csv_files:\n",
    "                df = pd.read_csv(file)\n",
    "                df['ts_event'] = pd.to_datetime(df['ts_event'])\n",
    "                df = df.sort_values('ts_event')\n",
    "                time_diffs = df['ts_event'].diff().dt.total_seconds()\n",
    "                all_diffs.extend(time_diffs.dropna().tolist())\n",
    "            \n",
    "            if not all_diffs:\n",
    "                raise ValueError(\"No valid time differences found in the data\")\n",
    "            \n",
    "            # Calculate statistics\n",
    "            median_diff = np.median(all_diffs)\n",
    "            mean_diff = np.mean(all_diffs)\n",
    "            std_diff = np.std(all_diffs)\n",
    "            \n",
    "            # Set sequence and prediction lengths based on data characteristics\n",
    "            # Sequence length: typical number of observations in 30 minutes\n",
    "            typical_observations_per_30min = int((30 * 60) / median_diff)\n",
    "            cls.sequence_length = min(max(typical_observations_per_30min, 10), 100)\n",
    "            \n",
    "            # Prediction length: typical number of observations in 5 minutes\n",
    "            typical_observations_per_5min = int((5 * 60) / median_diff)\n",
    "            cls.prediction_length = min(max(typical_observations_per_5min, 5), 30)\n",
    "            \n",
    "            print(f\"\\nTime Series Analysis Results:\")\n",
    "            print(f\"Median time between observations: {median_diff:.2f} seconds\")\n",
    "            print(f\"Mean time between observations: {mean_diff:.2f} seconds\")\n",
    "            print(f\"Standard deviation: {std_diff:.2f} seconds\")\n",
    "            print(f\"Selected sequence length: {cls.sequence_length} observations\")\n",
    "            print(f\"Selected prediction length: {cls.prediction_length} observations\")\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing time series: {str(e)}\")\n",
    "            # Set default values if analysis fails\n",
    "            cls.sequence_length = 100\n",
    "            cls.prediction_length = 30\n",
    "            return False\n",
    "\n",
    "    @classmethod\n",
    "    def initialize(cls):\n",
    "        \"\"\"Initialize configuration and validate settings\"\"\"\n",
    "        if not cls.validate_dates():\n",
    "            raise ValueError(\"Date validation failed\")\n",
    "        if not cls.analyze_time_series():\n",
    "            print(\"Warning: Using default sequence and prediction lengths\")\n",
    "        return True\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Defintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PricePredictionLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(PricePredictionLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        \n",
    "        # Add batch normalization\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        # Add dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(1)  # Add time dimension if not present\n",
    "            \n",
    "        # LSTM forward pass\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Apply batch normalization\n",
    "        out = self.batch_norm(out[:, -1, :])\n",
    "        \n",
    "        # Apply dropout\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Decode the hidden state\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, sequence_length=60):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: Normalized DataFrame with timestamp index and features\n",
    "            sequence_length: Number of time steps to look back\n",
    "        \"\"\"\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        # Convert DataFrame to numpy array\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            self.data = data.values\n",
    "        else:\n",
    "            self.data = data\n",
    "            \n",
    "        # Create sequences\n",
    "        self.sequences = self._create_sequences()\n",
    "    \n",
    "    def _create_sequences(self):\n",
    "        sequences = []\n",
    "        for i in range(len(self.data) - self.sequence_length):\n",
    "            # Get sequence of prices and features\n",
    "            sequence = self.data[i:(i + self.sequence_length)]\n",
    "            target = self.data[i + self.sequence_length, 0]  # Price is first column\n",
    "            sequences.append((sequence, target))\n",
    "        return sequences\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence, target = self.sequences[idx]\n",
    "        return torch.FloatTensor(sequence), torch.FloatTensor([target])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                num_epochs, device, patience=10):\n",
    "    \"\"\"Train the model with early stopping.\"\"\"\n",
    "    best_val_loss = float('inf')\n",
    "    no_improve = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for sequences, targets in train_loader:\n",
    "            sequences, targets = sequences.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for sequences, targets in val_loader:\n",
    "                sequences, targets = sequences.to(device), targets.to(device)\n",
    "                outputs = model(sequences)\n",
    "                val_loss += criterion(outputs, targets).item()\n",
    "        \n",
    "        # Print progress\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print(f'Training Loss: {train_loss/len(train_loader):.4f}')\n",
    "        print(f'Validation Loss: {val_loss/len(val_loader):.4f}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_file(file_path):\n",
    "    \"\"\"Process a single CSV file with proper timestamp and price handling\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Convert timestamp string to datetime\n",
    "    df['ts_event'] = pd.to_datetime(df['ts_event'], format='%Y-%m-%dT%H:%M:%S.%fZ', utc=True)\n",
    "    # Convert to local timezone if needed\n",
    "    # df['ts_event'] = df['ts_event'].dt.tz_convert('Your_Timezone')\n",
    "    \n",
    "    # Ensure price is float\n",
    "    df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    df = df.sort_values('ts_event')\n",
    "    \n",
    "    return df[['ts_event', 'price']]\n",
    "def process_csv_files(config):\n",
    "    \"\"\"Process all CSV files in the data directory.\"\"\"\n",
    "    all_data = []\n",
    "    csv_files = glob.glob(str(Path(config.DATA_DIR) / \"*.csv\"))\n",
    "    \n",
    "    if not csv_files:\n",
    "        raise ValueError(f\"No CSV files found in {config.DATA_DIR}\")\n",
    "    \n",
    "    for file in csv_files:\n",
    "        # Read CSV with proper timestamp parsing\n",
    "        df = pd.read_csv(file)\n",
    "        df['ts_event'] = pd.to_datetime(df['ts_event'])\n",
    "        df = df.set_index('ts_event')\n",
    "        df = df.sort_index()  # Sort by timestamp\n",
    "        all_data.append(df)\n",
    "    \n",
    "    # Combine all data and sort by timestamp\n",
    "    combined_df = pd.concat(all_data)\n",
    "    combined_df = combined_df.sort_index()\n",
    "    \n",
    "    # Convert config dates to pandas datetime\n",
    "    train_start = pd.to_datetime(config.TRAIN_START_DATE)\n",
    "    train_end = pd.to_datetime(config.TRAIN_END_DATE)\n",
    "    test_start = pd.to_datetime(config.TEST_START_DATE)\n",
    "    test_end = pd.to_datetime(config.TEST_END_DATE)\n",
    "    \n",
    "    # Split into train and test\n",
    "    train_data = combined_df[(combined_df.index >= train_start) & \n",
    "                            (combined_df.index <= train_end)].copy()\n",
    "    \n",
    "    test_data = combined_df[(combined_df.index >= test_start) & \n",
    "                           (combined_df.index <= test_end)].copy()\n",
    "    \n",
    "    return train_data, test_data\n",
    "def prepare_data(train_data, test_data, sequence_length=60):\n",
    "    \"\"\"Prepare data for LSTM model.\"\"\"\n",
    "    # Add technical indicators\n",
    "    def add_features(df):\n",
    "        df['returns'] = df['price'].pct_change()\n",
    "        df['log_returns'] = np.log1p(df['returns'])\n",
    "        \n",
    "        # Add rolling statistics\n",
    "        for window in [5, 10, 20]:\n",
    "            df[f'ma_{window}'] = df['price'].rolling(window=window).mean()\n",
    "            df[f'std_{window}'] = df['price'].rolling(window=window).std()\n",
    "            \n",
    "        # Add time features\n",
    "        df['hour'] = df.index.hour\n",
    "        df['minute'] = df.index.minute\n",
    "        df['day_of_week'] = df.index.dayofweek\n",
    "        \n",
    "        return df.fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    train_data = add_features(train_data)\n",
    "    test_data = add_features(test_data)\n",
    "    \n",
    "    # Scale the data\n",
    "    scaler = MinMaxScaler()\n",
    "    train_scaled = scaler.fit_transform(train_data)\n",
    "    test_scaled = scaler.transform(test_data)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TimeSeriesDataset(train_scaled, sequence_length)\n",
    "    test_dataset = TimeSeriesDataset(test_scaled, sequence_length)\n",
    "    \n",
    "    return train_dataset, test_dataset, scaler\n",
    "\n",
    "def create_data_loaders(train_dataset, test_dataset, batch_size=32, num_workers=0):\n",
    "    \"\"\"Create DataLoaders for training and testing.\"\"\"\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visuals and pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "def add_technical_features(df):\n",
    "    \"\"\"Add technical indicators as features.\"\"\"\n",
    "    # Convert only numeric columns to float32\n",
    "    numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    df[numeric_columns] = df[numeric_columns].astype('float32')\n",
    "    \n",
    "    df['returns'] = df['price'].pct_change()\n",
    "    \n",
    "    windows = [5, 15, 30, 60]\n",
    "    for window in windows:\n",
    "        df[f'sma_{window}'] = df['price'].rolling(window=window).mean()\n",
    "        df[f'std_{window}'] = df['price'].rolling(window=window).std()\n",
    "    \n",
    "    # Time-based features should be handled separately\n",
    "    df['hour'] = df.index.hour\n",
    "    df['minute'] = df.index.minute\n",
    "    df['day_of_week'] = df.index.dayofweek\n",
    "    \n",
    "    # Ensure numeric columns are float32\n",
    "    numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    df[numeric_columns] = df[numeric_columns].astype('float32')\n",
    "    \n",
    "    df = df.ffill().bfill()    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_training_environment(config):\n",
    "    \"\"\"Initialize training environment and device\"\"\"\n",
    "    torch.manual_seed(config.RANDOM_SEED)\n",
    "    np.random.seed(config.RANDOM_SEED)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(f\"Using device: {device}\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model, scaled_data, data_scaler, config, device):\n",
    "    \"\"\"Generate model predictions\"\"\"\n",
    "    model.eval()\n",
    "    predictions_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Ensure scaled_data is a numpy array of float type\n",
    "        scaled_data = np.array(scaled_data, dtype=np.float32)\n",
    "        \n",
    "        # Get the last sequence from training data\n",
    "        last_sequence = scaled_data[-config.sequence_length:].reshape(1, config.sequence_length, -1)\n",
    "        last_sequence = torch.FloatTensor(last_sequence).to(device)\n",
    "        current_sequence = last_sequence\n",
    "        \n",
    "        # Generate predictions\n",
    "        for _ in range(config.prediction_length):\n",
    "            pred = model(current_sequence)\n",
    "            predictions_list.append(pred.cpu().numpy()[0])\n",
    "            \n",
    "            # Update sequence for next prediction\n",
    "            new_sequence = current_sequence.clone()\n",
    "            new_sequence = new_sequence[:, 1:, :]\n",
    "            new_row = torch.zeros((1, 1, scaled_data.shape[1])).to(device)\n",
    "            new_row[:, :, 0] = pred\n",
    "            current_sequence = torch.cat([new_sequence, new_row], dim=1)\n",
    "    \n",
    "    # Convert predictions back to original scale\n",
    "    predictions = np.array(predictions_list)\n",
    "    predictions_reshaped = np.zeros((len(predictions), scaled_data.shape[1]))\n",
    "    predictions_reshaped[:, 0] = predictions.flatten()\n",
    "    predicted_prices = data_scaler.inverse_transform(predictions_reshaped)[:, 0]\n",
    "    \n",
    "    return predicted_prices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_timestamps(df):\n",
    "    \"\"\"\n",
    "    Process timestamps from ISO format and ensure proper time series continuity\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with ts_event column in ISO format\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame with proper timestamps\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert ISO timestamps to pandas datetime if not already datetime\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['ts_event']):\n",
    "        df['ts_event'] = pd.to_datetime(df['ts_event'], format='%Y-%m-%dT%H:%M:%S.%fZ', utc=True)\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    df = df.sort_values('ts_event')\n",
    "    \n",
    "    # Calculate time differences between consecutive timestamps\n",
    "    df['time_diff'] = df['ts_event'].diff()\n",
    "    \n",
    "    # Add timestamp-based features\n",
    "    df['hour'] = df['ts_event'].dt.hour\n",
    "    df['minute'] = df['ts_event'].dt.minute\n",
    "    df['second'] = df['ts_event'].dt.second\n",
    "    df['microsecond'] = df['ts_event'].dt.microsecond\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_training_data(train_df, config):\n",
    "    \"\"\"Prepare data for training with consistent feature handling\"\"\"\n",
    "    try:\n",
    "        # Store original data before modifications\n",
    "        original_data = {\n",
    "            'timestamp': train_df.index,\n",
    "            'price': train_df['price'].copy()\n",
    "        }\n",
    "        \n",
    "        print(\"Adding technical features...\")\n",
    "        # Add technical features while keeping track of feature names\n",
    "        feature_columns = ['price']  # Start with price as first feature\n",
    "        \n",
    "        # Calculate returns\n",
    "        train_df['returns'] = train_df['price'].pct_change()\n",
    "        feature_columns.append('returns')\n",
    "        \n",
    "        # Add rolling statistics\n",
    "        windows = [5, 15, 30, 60]\n",
    "        for window in windows:\n",
    "            train_df[f'sma_{window}'] = train_df['price'].rolling(window=window).mean()\n",
    "            train_df[f'std_{window}'] = train_df['price'].rolling(window=window).std()\n",
    "            feature_columns.extend([f'sma_{window}', f'std_{window}'])\n",
    "        \n",
    "        # Fill NaN values\n",
    "        train_df = train_df.fillna(method='ffill').fillna(method='bfill')\n",
    "        \n",
    "        print(\"Scaling data...\")\n",
    "        # Scale only the numeric features\n",
    "        scaler = MinMaxScaler()\n",
    "        scaled_data = scaler.fit_transform(train_df[feature_columns])\n",
    "        \n",
    "        # Store feature names with the scaler for later use\n",
    "        scaler.feature_names_ = feature_columns\n",
    "        \n",
    "        return scaled_data, scaler, pd.DataFrame(original_data)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in prepare_training_data: {str(e)}\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "def prepare_data_for_visualization(model, train_df, test_data, config, scaler):\n",
    "    \"\"\"Prepare data for visualization with consistent feature handling\"\"\"\n",
    "    try:\n",
    "        # Get test data with a small buffer\n",
    "        test_start = pd.to_datetime(config.TEST_START_DATE, utc=True) - pd.Timedelta(microseconds=1)\n",
    "        test_end = pd.to_datetime(config.TEST_END_DATE, utc=True)\n",
    "        \n",
    "        # Filter test data\n",
    "        test_data = train_df[\n",
    "            (train_df.index > test_start) & \n",
    "            (train_df.index <= test_end)\n",
    "        ].copy()\n",
    "        \n",
    "        # Validate input data\n",
    "        if test_data.empty:\n",
    "            print(f\"Test period: {test_start} to {test_end}\")\n",
    "            print(f\"Available data range: {train_df.index.min()} to {train_df.index.max()}\")\n",
    "            raise ValueError(\"Test data is empty. Check your date ranges.\")\n",
    "            \n",
    "        # Print data ranges for debugging\n",
    "        print(f\"Test data range: {test_data.index.min()} to {test_data.index.max()}\")\n",
    "        print(f\"Test data shape: {test_data.shape}\")\n",
    "        \n",
    "        # Create DataFrame for visualization\n",
    "        actual_data = pd.DataFrame({\n",
    "            'ts_event': test_data.index,\n",
    "            'price': test_data['price'],\n",
    "            'type': 'actual'\n",
    "        })\n",
    "        \n",
    "        # Generate predictions\n",
    "        train_data_numpy = train_df.select_dtypes(include=[np.number]).values\n",
    "        predicted_prices = generate_predictions(model, train_data_numpy, scaler, config, next(iter(model.parameters())).device)\n",
    "        \n",
    "        # Create predictions DataFrame\n",
    "        predictions_df = pd.DataFrame({\n",
    "            'ts_event': pd.date_range(\n",
    "                start=test_data.index[0],\n",
    "                periods=len(predicted_prices),\n",
    "                freq='1min'\n",
    "            ),\n",
    "            'price': predicted_prices,\n",
    "            'type': 'predicted'\n",
    "        })\n",
    "        \n",
    "        return actual_data, predictions_df\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in prepare_data_for_visualization: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def plot_predictions_with_actual(actual_data, predictions_df, config):\n",
    "    \"\"\"Plot actual vs predicted prices with proper timestamp handling\"\"\"\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Make sure timestamps are datetime\n",
    "    actual_data['ts_event'] = pd.to_datetime(actual_data['ts_event'])\n",
    "    predictions_df['ts_event'] = pd.to_datetime(predictions_df['ts_event'])\n",
    "    \n",
    "    # Plot actual prices\n",
    "    actual_mask = actual_data['type'] == 'actual'\n",
    "    if any(actual_mask):\n",
    "        plt.plot(actual_data[actual_mask]['ts_event'],\n",
    "                actual_data[actual_mask]['price'],\n",
    "                label='Actual Price',\n",
    "                color='green',\n",
    "                linewidth=2)\n",
    "        \n",
    "        # Print actual price stats\n",
    "        print(\"\\nActual Price Statistics:\")\n",
    "        print(actual_data[actual_mask]['price'].describe())\n",
    "    \n",
    "    # Plot predicted prices\n",
    "    predicted_mask = predictions_df['type'] == 'predicted'\n",
    "    if any(predicted_mask):\n",
    "        plt.plot(predictions_df[predicted_mask]['ts_event'],\n",
    "                predictions_df[predicted_mask]['price'],\n",
    "                label='Predicted Price',\n",
    "                color='blue',\n",
    "                linewidth=2,\n",
    "                linestyle='--')\n",
    "        \n",
    "        # Print predicted price stats\n",
    "        print(\"\\nPredicted Price Statistics:\")\n",
    "        print(predictions_df[predicted_mask]['price'].describe())\n",
    "    \n",
    "    # Add vertical line at prediction start\n",
    "    pred_start = pd.to_datetime(config.TEST_START_DATE, format='%Y-%m-%dT%H:%M:%S.%fZ', utc=True)\n",
    "    plt.axvline(x=pred_start, color='red', linestyle=':', label='Prediction Start')\n",
    "    \n",
    "    plt.title('Price Prediction Analysis', fontsize=16, pad=20)\n",
    "    plt.xlabel('Time', fontsize=12)\n",
    "    plt.ylabel('Price ($)', fontsize=12)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Format axes\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    plt.gca().yaxis.set_major_formatter(plt.FormatStrFormatter('%.6f'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_data(config):\n",
    "    \"\"\"Load and process all data files with proper timestamp handling\"\"\"\n",
    "    all_data = []\n",
    "    csv_files = glob.glob(str(Path(config.DATA_DIR) / \"*.csv\"))\n",
    "    \n",
    "    if not csv_files:\n",
    "        raise ValueError(f\"No CSV files found in directory: {config.DATA_DIR}\")\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files\")\n",
    "    print(\"Processing data files...\")\n",
    "    \n",
    "    for file in tqdm(csv_files):\n",
    "        try:\n",
    "            # Read CSV\n",
    "            df = pd.read_csv(file)\n",
    "            \n",
    "            # Convert timestamps properly without trying to localize\n",
    "            df['ts_event'] = pd.to_datetime(df['ts_event'], utc=True)\n",
    "            \n",
    "            # Convert config dates to datetime with UTC timezone\n",
    "            train_start = pd.to_datetime(config.TRAIN_START_DATE, utc=True)\n",
    "            train_end = pd.to_datetime(config.TRAIN_END_DATE, utc=True)\n",
    "            \n",
    "            # Add a small buffer to avoid exact timestamp matching issues\n",
    "            train_start = train_start - pd.Timedelta(seconds=1)\n",
    "            train_end = train_end + pd.Timedelta(seconds=1)\n",
    "            \n",
    "            # Filter data\n",
    "            mask = (df['ts_event'] >= train_start) & (df['ts_event'] <= train_end)\n",
    "            df = df[mask]\n",
    "            \n",
    "            if not df.empty:\n",
    "                all_data.append(df)\n",
    "                print(f\"File {Path(file).name}: Found {len(df)} records in time range\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file}: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    if not all_data:\n",
    "        raise ValueError(\n",
    "            f\"No data found between {config.TRAIN_START_DATE} and {config.TRAIN_END_DATE}\\n\"\n",
    "            f\"Please check your date ranges and data directory: {config.DATA_DIR}\"\n",
    "        )\n",
    "    \n",
    "    # Combine all data\n",
    "    combined_df = pd.concat(all_data)\n",
    "    combined_df = combined_df.sort_values('ts_event')\n",
    "    \n",
    "    # Print data range information\n",
    "    print(f\"\\nData range: {combined_df['ts_event'].min()} to {combined_df['ts_event'].max()}\")\n",
    "    print(f\"Total records: {len(combined_df)}\")\n",
    "    \n",
    "    # Create regular time intervals if needed\n",
    "    print(\"\\nResampling to regular intervals...\")\n",
    "    combined_df.set_index('ts_event', inplace=True)\n",
    "    # Resample to 1-minute intervals\n",
    "    combined_df = combined_df.resample('1min').last().ffill()\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(scaled_data, config):\n",
    "    \"\"\"Create train and validation data loaders\"\"\"\n",
    "    train_size = int(config.TRAIN_VAL_SPLIT * len(scaled_data))\n",
    "    \n",
    "    train_dataset = TimeSeriesDataset(scaled_data[:train_size], sequence_length=config.sequence_length)\n",
    "    val_dataset = TimeSeriesDataset(scaled_data[train_size:], sequence_length=config.sequence_length)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparison_dataframes(predicted_prices, actual_data, config):\n",
    "    \"\"\"Create DataFrames for comparison\"\"\"\n",
    "    last_timestamp = pd.to_datetime(config.TEST_START_DATE)\n",
    "    \n",
    "    # Create timestamp index for predictions\n",
    "    pred_index = pd.date_range(\n",
    "        start=last_timestamp,\n",
    "        periods=config.prediction_length + 1,\n",
    "        freq='1min'\n",
    "    )[1:]\n",
    "    \n",
    "    predictions_df = pd.DataFrame({\n",
    "        'timestamp': pred_index,\n",
    "        'price': predicted_prices,\n",
    "        'type': 'predicted'\n",
    "    })\n",
    "    \n",
    "    # Get actual prices for comparison\n",
    "    actual_prices = actual_data[\n",
    "        (actual_data['timestamp'] <= pd.to_datetime(config.TEST_END_DATE)) &\n",
    "        (actual_data['timestamp'] >= pd.to_datetime(config.TEST_START_DATE))\n",
    "    ].copy()\n",
    "    \n",
    "    actual_prices = actual_prices.rename(columns={'actual_price': 'price'})\n",
    "    actual_prices['type'] = 'actual'\n",
    "    \n",
    "    # Combine actual and predicted\n",
    "    comparison_df = pd.concat([actual_prices, predictions_df], axis=0)\n",
    "    comparison_df = comparison_df.sort_values('timestamp').reset_index(drop=True)\n",
    "    \n",
    "    return predictions_df, comparison_df\n",
    "\n",
    "def create_prediction_timestamps(config, n_predictions):\n",
    "    \"\"\"Create properly spaced timestamps for predictions\"\"\"\n",
    "    # Convert test start date to datetime\n",
    "    test_start = pd.to_datetime(config.TEST_START_DATE, format='%Y-%m-%dT%H:%M:%S.%fZ', utc=True)\n",
    "    \n",
    "    # Create timestamp range for predictions\n",
    "    pred_timestamps = pd.date_range(\n",
    "        start=test_start,\n",
    "        periods=n_predictions + 1,  # +1 to include start\n",
    "        freq='1min',  # adjust frequency as needed\n",
    "        tz='UTC'\n",
    "    )\n",
    "    \n",
    "    return pred_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(comparison_df, config):\n",
    "    \"\"\"Plot actual vs predicted prices with proper timestamp handling\"\"\"\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Convert timestamps to datetime if they aren't already\n",
    "    comparison_df['timestamp'] = pd.to_datetime(comparison_df['timestamp'])\n",
    "    \n",
    "    # Plot actual prices\n",
    "    actual_mask = comparison_df['type'] == 'actual'\n",
    "    if any(actual_mask):\n",
    "        plt.plot(comparison_df[actual_mask]['timestamp'],\n",
    "                comparison_df[actual_mask]['price'],\n",
    "                label='Actual Price',\n",
    "                color='green',\n",
    "                linewidth=2)\n",
    "    \n",
    "    # Plot predicted prices\n",
    "    predicted_mask = comparison_df['type'] == 'predicted'\n",
    "    if any(predicted_mask):\n",
    "        plt.plot(comparison_df[predicted_mask]['timestamp'],\n",
    "                comparison_df[predicted_mask]['price'],\n",
    "                label='Predicted Price',\n",
    "                color='blue',\n",
    "                linewidth=2,\n",
    "                linestyle='--')\n",
    "    \n",
    "    # Add vertical line at prediction start\n",
    "    pred_start = pd.to_datetime(config.TEST_START_DATE, format='%Y-%m-%dT%H:%M:%S.%fZ', utc=True)\n",
    "    plt.axvline(x=pred_start, color='red', linestyle=':', label='Prediction Start')\n",
    "    \n",
    "    plt.title('Price Prediction Analysis', fontsize=16, pad=20)\n",
    "    plt.xlabel('Time', fontsize=12)\n",
    "    plt.ylabel('Price ($)', fontsize=12)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Format x-axis\n",
    "    plt.gcf().autofmt_xdate()  # Rotate and align x-axis labels\n",
    "    \n",
    "    # Use tight layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(comparison_df):\n",
    "    \"\"\"Calculate and display prediction metrics\"\"\"\n",
    "    actual_mask = comparison_df['type'] == 'actual'\n",
    "    predicted_mask = comparison_df['type'] == 'predicted'\n",
    "    overlap_timestamps = set(comparison_df[actual_mask]['timestamp']) & set(comparison_df[predicted_mask]['timestamp'])\n",
    "    \n",
    "    if overlap_timestamps:\n",
    "        overlap_df = comparison_df[comparison_df['timestamp'].isin(overlap_timestamps)].copy()\n",
    "        actual_values = overlap_df[actual_mask]['price'].values\n",
    "        predicted_values = overlap_df[predicted_mask]['price'].values\n",
    "        \n",
    "        metrics = {\n",
    "            'MSE': np.mean((actual_values - predicted_values) ** 2),\n",
    "            'RMSE': np.sqrt(np.mean((actual_values - predicted_values) ** 2)),\n",
    "            'MAE': np.mean(np.abs(actual_values - predicted_values)),\n",
    "            'MAPE': np.mean(np.abs((actual_values - predicted_values) / actual_values)) * 100,\n",
    "            'Correlation': np.corrcoef(actual_values, predicted_values)[0, 1] if len(actual_values) > 1 else np.nan\n",
    "        }\n",
    "        \n",
    "        metrics_df = pd.DataFrame([metrics]).T.round(4)\n",
    "        metrics_df.columns = ['Value']\n",
    "        \n",
    "        print(\"\\nPrediction Metrics:\")\n",
    "        display(metrics_df)\n",
    "        \n",
    "        return metrics_df\n",
    "    else:\n",
    "        print(\"\\nNo overlap period found between actual and predicted prices.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    \"\"\"Main training and prediction pipeline\"\"\"\n",
    "    try:\n",
    "        # Initialize configuration and environment\n",
    "        if not config.initialize():\n",
    "            raise ValueError(\"Configuration initialization failed\")\n",
    "        device = initialize_training_environment(config)\n",
    "        \n",
    "        print(\"\\nInitialization complete:\")\n",
    "        print(f\"Sequence length: {config.sequence_length}\")\n",
    "        print(f\"Prediction length: {config.prediction_length}\")\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        # Load and process data\n",
    "        print(\"\\nLoading and processing data...\")\n",
    "        train_df = load_and_process_data(config)\n",
    "        if train_df is None or train_df.empty:\n",
    "            raise ValueError(\"No training data loaded\")\n",
    "        \n",
    "        # Print data range information\n",
    "        print(f\"\\nLoaded data range: {train_df.index.min()} to {train_df.index.max()}\")\n",
    "        print(f\"Number of training samples: {len(train_df)}\")\n",
    "        \n",
    "        print(\"\\nPreparing training data...\")\n",
    "        scaled_data, data_scaler, actual_data = prepare_training_data(train_df, config)\n",
    "        if scaled_data is None:\n",
    "            raise ValueError(\"Failed to scale data\")\n",
    "        \n",
    "        print(\"\\nCreating data loaders...\")\n",
    "        train_loader, val_loader = create_data_loaders(scaled_data, config)\n",
    "        \n",
    "        # Initialize model\n",
    "        print(\"\\nInitializing model...\")\n",
    "        input_size = scaled_data.shape[1]  # Number of features\n",
    "        model = PricePredictionLSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=config.HIDDEN_SIZE,\n",
    "            num_layers=config.NUM_LAYERS\n",
    "        ).to(device)\n",
    "        \n",
    "        # Setup training\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n",
    "        \n",
    "        # Train model\n",
    "        print(f\"\\nStarting training for {config.EPOCHS} epochs...\")\n",
    "        train_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                   config.EPOCHS, device, config.PATIENCE)\n",
    "        \n",
    "        # Prepare test data with buffer\n",
    "        test_start = pd.to_datetime(config.TEST_START_DATE, utc=True) - pd.Timedelta(microseconds=1)\n",
    "        test_end = pd.to_datetime(config.TEST_END_DATE, utc=True) + pd.Timedelta(microseconds=1)\n",
    "        \n",
    "        test_data = train_df[\n",
    "            (train_df.index > test_start) & \n",
    "            (train_df.index <= test_end)\n",
    "        ].copy()\n",
    "        \n",
    "        if test_data.empty:\n",
    "            print(f\"\\nTest period: {test_start} to {test_end}\")\n",
    "            print(f\"Available data range: {train_df.index.min()} to {train_df.index.max()}\")\n",
    "            raise ValueError(f\"No test data found in specified range\")\n",
    "        \n",
    "        print(f\"\\nTest data range: {test_data.index.min()} to {test_data.index.max()}\")\n",
    "        print(f\"Number of test samples: {len(test_data)}\")\n",
    "        \n",
    "        # Generate predictions\n",
    "        print(\"\\nGenerating predictions...\")\n",
    "        actual_data_test, predictions_df = prepare_data_for_visualization(\n",
    "            model, train_df, test_data, config, data_scaler\n",
    "        )\n",
    "        \n",
    "        if predictions_df is None or predictions_df.empty:\n",
    "            raise ValueError(\"Failed to generate predictions\")\n",
    "        \n",
    "        # Visualize results\n",
    "        print(\"\\nVisualizing results...\")\n",
    "        plot_predictions_with_actual(actual_data_test, predictions_df, config)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        print(\"\\nCalculating metrics...\")\n",
    "        comparison_df = prepare_comparison_dataframe(actual_data_test, predictions_df)\n",
    "        metrics_df = calculate_metrics(comparison_df)\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(\"\\nSummary Statistics:\")\n",
    "        print_summary_statistics(comparison_df)\n",
    "        \n",
    "        print(\"\\nPipeline completed successfully!\")\n",
    "        return model, comparison_df, metrics_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in main pipeline: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return None, None, None\n",
    "    \n",
    "    finally:\n",
    "        # Cleanup\n",
    "        try:\n",
    "            torch.cuda.empty_cache()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "def run_pipeline(config_class=Config):\n",
    "    \"\"\"Wrapper function to run the main pipeline with proper setup\"\"\"\n",
    "    try:\n",
    "        # Create Config instance if needed\n",
    "        config = config_class() if isinstance(config_class, type) else config_class\n",
    "            \n",
    "        # Run main pipeline\n",
    "        model, comparison_df, metrics_df = main(config)\n",
    "        \n",
    "        if model is None:\n",
    "            print(\"\\nPipeline failed!\")\n",
    "            return None, None, None\n",
    "            \n",
    "        # Save results if successful\n",
    "        save_results(model, comparison_df, metrics_df)\n",
    "        \n",
    "        return model, comparison_df, metrics_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nFatal error in pipeline: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return None, None, None\n",
    "\n",
    "def prepare_comparison_dataframe(actual_data_test, predictions_df):\n",
    "    \"\"\"Prepare comparison DataFrame with consistent column names\"\"\"\n",
    "    if 'ts_event' not in actual_data_test.columns:\n",
    "        actual_data_test['ts_event'] = actual_data_test.index\n",
    "    if 'ts_event' not in predictions_df.columns:\n",
    "        predictions_df['ts_event'] = predictions_df.index\n",
    "        \n",
    "    comparison_df = pd.concat([actual_data_test, predictions_df])\n",
    "    return comparison_df.sort_values('ts_event').reset_index(drop=True)\n",
    "\n",
    "def print_summary_statistics(comparison_df):\n",
    "    \"\"\"Print summary statistics for actual and predicted prices\"\"\"\n",
    "    for data_type in ['actual', 'predicted']:\n",
    "        data_mask = comparison_df['type'] == data_type\n",
    "        if any(data_mask):\n",
    "            print(f\"\\n{data_type.title()} Price Statistics:\")\n",
    "            print(comparison_df[data_mask]['price'].describe())\n",
    "\n",
    "def save_results(model, comparison_df, metrics_df):\n",
    "    \"\"\"Save model, comparison results, and metrics\"\"\"\n",
    "    try:\n",
    "        torch.save(model.state_dict(), 'final_model.pth')\n",
    "        \n",
    "        if comparison_df is not None:\n",
    "            comparison_df_save = comparison_df.copy()\n",
    "            if 'ts_event' in comparison_df_save.columns:\n",
    "                comparison_df_save['ts_event'] = comparison_df_save['ts_event'].dt.strftime('%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "            comparison_df_save.to_csv('comparison_results.csv', index=False)\n",
    "        \n",
    "        if metrics_df is not None:\n",
    "            metrics_df.to_csv('metrics_results.csv')\n",
    "        \n",
    "        print(\"\\nResults saved successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nWarning: Failed to save results: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating date ranges:\n",
      "Train period: 2018-05-03 08:00:46+00:00 to 2018-05-03 20:00:00.000001+00:00\n",
      "Test period: 2018-05-03 20:00:00+00:00 to 2018-05-03 23:59:58.000001+00:00\n",
      "\n",
      "Time Series Analysis Results:\n",
      "Median time between observations: 0.00 seconds\n",
      "Mean time between observations: 1.69 seconds\n",
      "Standard deviation: 27.72 seconds\n",
      "Selected sequence length: 100 observations\n",
      "Selected prediction length: 30 observations\n",
      "Using device: cuda\n",
      "\n",
      "Initialization complete:\n",
      "Sequence length: 100\n",
      "Prediction length: 30\n",
      "Using device: cuda\n",
      "\n",
      "Loading and processing data...\n",
      "Found 2 CSV files\n",
      "Processing data files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:00<00:00, 14.54it/s]\n",
      "C:\\Users\\cinco\\AppData\\Local\\Temp\\ipykernel_30416\\3484768983.py:57: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  train_df = train_df.fillna(method='ffill').fillna(method='bfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File xnas.itch_NVDA_20180503_to_20180504.csv: Found 41983 records in time range\n",
      "\n",
      "Data range: 2018-05-03 08:00:46.643023992+00:00 to 2018-05-03 20:00:00.225802840+00:00\n",
      "Total records: 41983\n",
      "\n",
      "Resampling to regular intervals...\n",
      "\n",
      "Loaded data range: 2018-05-03 08:00:00+00:00 to 2018-05-03 20:00:00+00:00\n",
      "Number of training samples: 721\n",
      "\n",
      "Preparing training data...\n",
      "Adding technical features...\n",
      "Scaling data...\n",
      "\n",
      "Creating data loaders...\n",
      "\n",
      "Initializing model...\n",
      "\n",
      "Starting training for 50 epochs...\n",
      "Epoch 1/50\n",
      "Training Loss: 0.3244\n",
      "Validation Loss: 0.9766\n",
      "Epoch 2/50\n",
      "Training Loss: 0.2202\n",
      "Validation Loss: 0.8382\n",
      "Epoch 3/50\n",
      "Training Loss: 0.1557\n",
      "Validation Loss: 0.6840\n",
      "Epoch 4/50\n",
      "Training Loss: 0.1355\n",
      "Validation Loss: 0.5724\n",
      "Epoch 5/50\n",
      "Training Loss: 0.0980\n",
      "Validation Loss: 0.3591\n",
      "Epoch 6/50\n",
      "Training Loss: 0.0849\n",
      "Validation Loss: 0.1177\n",
      "Epoch 7/50\n",
      "Training Loss: 0.0735\n",
      "Validation Loss: 0.0381\n",
      "Epoch 8/50\n",
      "Training Loss: 0.0629\n",
      "Validation Loss: 0.0513\n",
      "Epoch 9/50\n",
      "Training Loss: 0.0573\n",
      "Validation Loss: 0.0110\n",
      "Epoch 10/50\n",
      "Training Loss: 0.0491\n",
      "Validation Loss: 0.0300\n",
      "Epoch 11/50\n",
      "Training Loss: 0.0445\n",
      "Validation Loss: 0.0044\n",
      "Epoch 12/50\n",
      "Training Loss: 0.0420\n",
      "Validation Loss: 0.0006\n",
      "Epoch 13/50\n",
      "Training Loss: 0.0443\n",
      "Validation Loss: 0.0077\n",
      "Epoch 14/50\n",
      "Training Loss: 0.0393\n",
      "Validation Loss: 0.0414\n",
      "Epoch 15/50\n",
      "Training Loss: 0.0342\n",
      "Validation Loss: 0.0170\n",
      "Epoch 16/50\n",
      "Training Loss: 0.0278\n",
      "Validation Loss: 0.0015\n",
      "Epoch 17/50\n",
      "Training Loss: 0.0292\n",
      "Validation Loss: 0.0005\n",
      "Epoch 18/50\n",
      "Training Loss: 0.0212\n",
      "Validation Loss: 0.0062\n",
      "Epoch 19/50\n",
      "Training Loss: 0.0225\n",
      "Validation Loss: 0.0015\n",
      "Epoch 20/50\n",
      "Training Loss: 0.0193\n",
      "Validation Loss: 0.0019\n",
      "Epoch 21/50\n",
      "Training Loss: 0.0184\n",
      "Validation Loss: 0.0005\n",
      "Epoch 22/50\n",
      "Training Loss: 0.0158\n",
      "Validation Loss: 0.0261\n",
      "Epoch 23/50\n",
      "Training Loss: 0.0201\n",
      "Validation Loss: 0.0010\n",
      "Epoch 24/50\n",
      "Training Loss: 0.0169\n",
      "Validation Loss: 0.0213\n",
      "Epoch 25/50\n",
      "Training Loss: 0.0131\n",
      "Validation Loss: 0.0019\n",
      "Epoch 26/50\n",
      "Training Loss: 0.0120\n",
      "Validation Loss: 0.0093\n",
      "Epoch 27/50\n",
      "Training Loss: 0.0116\n",
      "Validation Loss: 0.0080\n",
      "Epoch 28/50\n",
      "Training Loss: 0.0120\n",
      "Validation Loss: 0.0022\n",
      "Epoch 29/50\n",
      "Training Loss: 0.0095\n",
      "Validation Loss: 0.0010\n",
      "Epoch 30/50\n",
      "Training Loss: 0.0090\n",
      "Validation Loss: 0.0040\n",
      "Epoch 31/50\n",
      "Training Loss: 0.0098\n",
      "Validation Loss: 0.0048\n",
      "Early stopping triggered at epoch 31\n",
      "\n",
      "Test data range: 2018-05-03 20:00:00+00:00 to 2018-05-03 20:00:00+00:00\n",
      "Number of test samples: 1\n",
      "\n",
      "Generating predictions...\n",
      "Test data range: 2018-05-03 20:00:00+00:00 to 2018-05-03 20:00:00+00:00\n",
      "Test data shape: (1, 22)\n",
      "Error in prepare_data_for_visualization: input.size(-1) must be equal to input_size. Expected 10, got 18\n",
      "\n",
      "Error in main pipeline: Failed to generate predictions\n",
      "\n",
      "Pipeline failed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\cinco\\AppData\\Local\\Temp\\ipykernel_30416\\3484768983.py\", line 106, in prepare_data_for_visualization\n",
      "    predicted_prices = generate_predictions(model, train_data_numpy, scaler, config, next(iter(model.parameters())).device)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\cinco\\AppData\\Local\\Temp\\ipykernel_30416\\3343492228.py\", line 17, in generate_predictions\n",
      "    pred = model(current_sequence)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\cinco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\cinco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\cinco\\AppData\\Local\\Temp\\ipykernel_30416\\4044704372.py\", line 36, in forward\n",
      "    out, _ = self.lstm(x, (h0, c0))\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\cinco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\cinco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\cinco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py\", line 907, in forward\n",
      "    self.check_forward_args(input, hx, batch_sizes)\n",
      "  File \"c:\\Users\\cinco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py\", line 821, in check_forward_args\n",
      "    self.check_input(input, batch_sizes)\n",
      "  File \"c:\\Users\\cinco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py\", line 240, in check_input\n",
      "    raise RuntimeError(\n",
      "RuntimeError: input.size(-1) must be equal to input_size. Expected 10, got 18\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\cinco\\AppData\\Local\\Temp\\ipykernel_30416\\2346217178.py\", line 74, in main\n",
      "    raise ValueError(\"Failed to generate predictions\")\n",
      "ValueError: Failed to generate predictions\n"
     ]
    }
   ],
   "source": [
    "model, comparison_df, metrics_df = run_pipeline(Config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
